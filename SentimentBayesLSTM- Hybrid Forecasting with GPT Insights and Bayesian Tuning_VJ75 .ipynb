{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWqzDtMMoG5i"
      },
      "source": [
        "\n",
        "# SentimentBayesLSTM: Hybrid Forecasting with GPT Insights and Bayesian Tuning\n",
        "\n",
        "This project focuses on using LSTM models to predict stock prices based on historical data, sentiment analysis, and technical indicators. The code includes a base `StockPredictor` class, a standard `LSTMModel`, and an `ImprovedLSTMModel` with attention mechanisms and bidirectional LSTMs. Additionally, Bayesian optimization is applied to fine-tune the model's hyperparameters.\n",
        "\n",
        "## Import Libraries\n",
        "\n",
        "We will begin by importing the necessary libraries for data processing, model building, and visualization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRAYSrWUoUNH",
        "outputId": "b36dc0fb-1bd7-465f-8fba-402c604385ba"
      },
      "outputs": [],
      "source": [
        "!pip install numpy pandas scikit-learn tensorflow matplotlib requests beautifulsoup4 openai tqdm yfinance scikit-optimize ta\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkA1IbfCoG5k",
        "outputId": "b909aeb7-f1df-47e5-b566-9df3c73b3885"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, Attention, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.models import Model\n",
        "import matplotlib.dates as mdates\n",
        "from datetime import datetime, timedelta\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from openai import OpenAI\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import yfinance as yf\n",
        "from skopt import gp_minimize\n",
        "from skopt.space import Real, Integer\n",
        "from skopt.utils import use_named_args\n",
        "from tensorflow.keras import Input\n",
        "import ta\n",
        "import os\n",
        "import json\n",
        "import tensorflow as tf\n",
        "print(\"All libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52sLzYOaoG5l"
      },
      "source": [
        "## 1. Set Up OpenAI API\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "We will begin by importing the necessary libraries for data processing, model building, and OpenAI API interaction. You'll need to set your OpenAI API key to access GPT functionality. Please replace 'YOUR_OPENAI_API_KEY_HERE' with your actual OpenAI API key from https://platform.openai.com/api-keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJ2UcM5soG5l",
        "outputId": "d0606914-9801-4964-86e8-9a5df7c005f5"
      },
      "outputs": [],
      "source": [
        "api_key = 'YOUR_OPENAI_API_KEY_HERE'  # Replace with your actual OpenAI API key\n",
        "openai_client = OpenAI(api_key=api_key)\n",
        "\n",
        "def test_openai_client():\n",
        "    try:\n",
        "        response = openai_client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": \"Hello, are you working?\"}\n",
        "            ]\n",
        "        )\n",
        "        print(\"OpenAI client is working. Response:\", response.choices[0].message.content)\n",
        "    except Exception as e:\n",
        "        print(f\"Error testing OpenAI client: {str(e)}\")\n",
        "\n",
        "test_openai_client()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YLpNQT8oG5m"
      },
      "source": [
        "## 2. SentimentAnalysis Class\n",
        "\n",
        "This class handles sentiment analysis and is independent of the other classes. However, if we intend to integrate sentiment analysis as a feature in the stock prediction models, it should be defined before the `StockPredictor` class.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2qRdfc2oG5m",
        "outputId": "d1dbbaac-0ea2-4d91-f3ac-ee6d48a6e074"
      },
      "outputs": [],
      "source": [
        "class SentimentAnalyzer:\n",
        "    def __init__(self, openai_client):\n",
        "        self.openai_client = openai_client\n",
        "        self.cache = {}\n",
        "\n",
        "    def get_google_news_headlines(self, ticker, date):\n",
        "        url = f\"https://www.google.com/search?q={ticker}+stock&tbm=nws&source=lnt&tbs=cdr:1,cd_min:{date},cd_max:{date}\"\n",
        "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "        try:\n",
        "            response = requests.get(url, headers=headers)\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            headlines = soup.find_all('div', class_='BNeawe vvjwJb AP7Wnd')\n",
        "            return [headline.text for headline in headlines]\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching headlines: {e}\")\n",
        "            return []\n",
        "\n",
        "    def get_openai_sentiment_batch(self, headlines):\n",
        "        if not headlines:\n",
        "            return []\n",
        "\n",
        "        prompt = \"Analyze the sentiment of the following headlines and provide a sentiment score from -1 (very negative) to 1 (very positive) for each. Respond with only the numbers, separated by commas:\\n\\n\"\n",
        "        prompt += \"\\n\".join(headlines)\n",
        "\n",
        "        try:\n",
        "            response = self.openai_client.chat.completions.create(\n",
        "                model=\"gpt-4o-mini\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a financial sentiment analyzer. Always respond with numbers between -1 and 1, separated by commas.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                max_tokens=100\n",
        "            )\n",
        "\n",
        "            content = response.choices[0].message.content.strip()\n",
        "            sentiment_scores = [float(score.strip()) for score in content.split(',')]\n",
        "            return [max(min(score, 1), -1) for score in sentiment_scores]\n",
        "        except Exception as e:\n",
        "            print(f\"Error getting sentiment: {e}\")\n",
        "            return [0] * len(headlines)  # Return neutral sentiment in case of error\n",
        "\n",
        "    def add_weekly_sentiment(self, df, ticker):\n",
        "        cache_file = f\"{ticker}_sentiment_cache.json\"\n",
        "        df['Week'] = df.index.to_period('W')\n",
        "\n",
        "        if os.path.exists(cache_file):\n",
        "            with open(cache_file, 'r') as f:\n",
        "                self.cache = json.load(f)\n",
        "            print(f\"Loaded cached sentiment data for {ticker}\")\n",
        "        else:\n",
        "            self.cache = {}\n",
        "\n",
        "        weeks_to_process = [week for week in df['Week'].unique() if str(week) not in self.cache]\n",
        "        print(f\"Total weeks: {len(df['Week'].unique())}, Weeks to process: {len(weeks_to_process)}\")\n",
        "\n",
        "        for week in tqdm(weeks_to_process, desc=\"Fetching weekly sentiment\"):\n",
        "            week_str = str(week)\n",
        "            week_start = week.start_time.strftime('%Y-%m-%d')\n",
        "            week_end = week.end_time.strftime('%Y-%m-%d')\n",
        "            headlines = self.get_google_news_headlines(ticker, f\"{week_start},{week_end}\")\n",
        "            print(f\"Found {len(headlines)} headlines for week {week_str}\")\n",
        "\n",
        "            if headlines:\n",
        "                sentiments = self.get_openai_sentiment_batch(headlines)\n",
        "                sentiment = np.mean(sentiments)\n",
        "                self.cache[week_str] = sentiment\n",
        "                print(f\"Sentiment for week {week_str}: {sentiment}\")\n",
        "            else:\n",
        "                print(f\"No headlines found for week {week_str}, using neutral sentiment\")\n",
        "                self.cache[week_str] = 0\n",
        "\n",
        "            # Save updated cache after each new sentiment calculation\n",
        "            with open(cache_file, 'w') as f:\n",
        "                json.dump(self.cache, f)\n",
        "\n",
        "            time.sleep(3)  # Increased sleep time to 3 seconds\n",
        "\n",
        "        df['Sentiment'] = df['Week'].astype(str).map(self.cache)\n",
        "        df.drop('Week', axis=1, inplace=True)\n",
        "        return df\n",
        "\n",
        "print(\"Updated SentimentAnalyzer class with batch processing and increased sleep time defined successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXUbemyBoG5m"
      },
      "source": [
        "## 3. Define the StockPredictor Base Class\n",
        "\n",
        "This class will handle downloading stock data, adding technical indicators, and preparing data for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugoM2nzFoG5n",
        "outputId": "e1bd2013-dcba-405b-d035-a889e2b25520"
      },
      "outputs": [],
      "source": [
        "class StockPredictor:\n",
        "    def __init__(self, look_back=60):\n",
        "        self.look_back = look_back\n",
        "        self.scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "        self.feature_names = [\n",
        "            'Close', 'Sentiment', 'SMA20', 'SMA50', 'EMA20', 'RSI', 'MACD',\n",
        "            'MACD_Signal', 'Bollinger_High', 'Bollinger_Low', 'ATR', 'OBV',\n",
        "            'KAMA', 'Stochastic', 'Williams_R'\n",
        "        ]\n",
        "\n",
        "    def download_stock_data(self, ticker, start_date, end_date):\n",
        "        # Download historical stock data using yfinance.\n",
        "        stock = yf.Ticker(ticker)\n",
        "        df = stock.history(start=start_date, end=end_date)\n",
        "        return df\n",
        "\n",
        "    def add_technical_indicators(self, df):\n",
        "        # Calculate and add advanced technical indicators to the dataframe.\n",
        "        df['SMA20'] = ta.trend.sma_indicator(df['Close'], window=20)\n",
        "        df['SMA50'] = ta.trend.sma_indicator(df['Close'], window=50)\n",
        "        df['EMA20'] = ta.trend.ema_indicator(df['Close'], window=20)\n",
        "        df['RSI'] = ta.momentum.rsi(df['Close'], window=14)\n",
        "        macd = ta.trend.MACD(df['Close'])\n",
        "        df['MACD'] = macd.macd()\n",
        "        df['MACD_Signal'] = macd.macd_signal()\n",
        "        bollinger = ta.volatility.BollingerBands(df['Close'])\n",
        "        df['Bollinger_High'] = bollinger.bollinger_hband()\n",
        "        df['Bollinger_Low'] = bollinger.bollinger_lband()\n",
        "        df['ATR'] = ta.volatility.average_true_range(df['High'], df['Low'], df['Close'])\n",
        "        df['OBV'] = ta.volume.on_balance_volume(df['Close'], df['Volume'])\n",
        "\n",
        "        # Advanced indicators\n",
        "        df['KAMA'] = ta.momentum.kama(df['Close'], window=10)\n",
        "        df['Stochastic'] = ta.momentum.stoch(df['High'], df['Low'], df['Close'])\n",
        "        df['Williams_R'] = ta.momentum.williams_r(df['High'], df['Low'], df['Close'])\n",
        "\n",
        "        # Handle NaN values after adding indicators\n",
        "        df.ffill(inplace=True)  # Forward fill to handle NaNs\n",
        "        df.dropna(inplace=True)  # Drop any remaining NaNs\n",
        "        return df\n",
        "\n",
        "    def prepare_data(self, scaled_data):\n",
        "        x, y = [], []\n",
        "        for i in range(len(scaled_data) - self.look_back):\n",
        "            x.append(scaled_data[i:i+self.look_back])\n",
        "            y.append(scaled_data[i+self.look_back, 0])  # Assume 'Close' is the first column\n",
        "        return np.array(x), np.array(y).reshape(-1, 1)\n",
        "\n",
        "print(\"StockPredictor class defined successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHV5HH99oG5n"
      },
      "source": [
        "## 3. Define the `LSTMModel` Class with Hyperparameter Tuning and Uncertainty Estimation\n",
        "\n",
        "This version of the `LSTMModel` class extends `StockPredictor` and introduces several key features:\n",
        "- **Hyperparameter Tuning**: Bayesian optimization via `gp_minimize` to find the best values for LSTM units, dropout rate, and learning rate.\n",
        "- **Training**: The model is trained twice, first during hyperparameter tuning and then with the best parameters found.\n",
        "- **Uncertainty Estimation**: The `predict_with_uncertainty` method performs multiple forward passes with dropout enabled to estimate prediction uncertainty.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "MimRm-B7oG5n",
        "outputId": "302a1ce6-601f-4037-aa9c-83ccc1dc3019"
      },
      "outputs": [],
      "source": [
        "class LSTMModel(StockPredictor):\n",
        "    def __init__(self, look_back=60, forecast_horizon=1):\n",
        "        super().__init__(look_back)\n",
        "        self.forecast_horizon = forecast_horizon\n",
        "        self.model = self.build_model()\n",
        "\n",
        "    def build_model(self, lstm_units=128, dropout_rate=0.2, learning_rate=0.001):\n",
        "        lstm_units = int(lstm_units)\n",
        "        input_shape = (self.look_back, len(self.feature_names))\n",
        "\n",
        "        inputs = Input(shape=input_shape)\n",
        "        x = LSTM(units=lstm_units, return_sequences=True)(inputs)\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "        x = LSTM(units=lstm_units, return_sequences=True)(x)\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "        x = LSTM(units=lstm_units)(x)\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "        outputs = Dense(units=self.forecast_horizon)(x)\n",
        "\n",
        "        model = Model(inputs=inputs, outputs=outputs)\n",
        "        model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error')\n",
        "        print(\"Model Summary:\")\n",
        "        model.summary()\n",
        "        print(f\"Input shape: {input_shape}\")\n",
        "        return model\n",
        "\n",
        "    def train(self, x_train, y_train):\n",
        "        print(\"In train method:\")\n",
        "        print(f\"x_train shape: {x_train.shape}\")\n",
        "        print(f\"y_train shape: {y_train.shape}\")\n",
        "        print(f\"look_back: {self.look_back}\")\n",
        "        print(f\"Number of features: {len(self.feature_names)}\")\n",
        "\n",
        "        space = [\n",
        "            Integer(64, 256, name='lstm_units'),\n",
        "            Real(0.1, 0.5, name='dropout_rate'),\n",
        "            Real(1e-4, 1e-2, name='learning_rate', prior='log-uniform')\n",
        "        ]\n",
        "\n",
        "        @use_named_args(space)\n",
        "        def objective(**params):\n",
        "            model = self.build_model(**params)\n",
        "            try:\n",
        "                history = model.fit(\n",
        "                    x_train, y_train,\n",
        "                    epochs=50,\n",
        "                    batch_size=32,\n",
        "                    validation_split=0.2,\n",
        "                    verbose=0\n",
        "                )\n",
        "                return -history.history['val_loss'][-1]\n",
        "            except Exception as e:\n",
        "                print(f\"Error during model fitting: {str(e)}\")\n",
        "                return np.inf\n",
        "\n",
        "        result = gp_minimize(objective, space, n_calls=50, random_state=42)\n",
        "\n",
        "        best_params = {\n",
        "            'lstm_units': int(result.x[0]),\n",
        "            'dropout_rate': result.x[1],\n",
        "            'learning_rate': result.x[2]\n",
        "        }\n",
        "        print(\"Best parameters found: \", best_params)\n",
        "        print(\"Best score found: \", -result.fun)\n",
        "\n",
        "        self.model = self.build_model(**best_params)\n",
        "        self.model.fit(x_train, y_train, epochs=200, batch_size=32, verbose=1)\n",
        "    def predict(self, x_test):\n",
        "            \"\"\"\n",
        "            Make predictions using the trained LSTM model.\n",
        "            :param x_test: Test data\n",
        "            :return: Predictions from the model\n",
        "            \"\"\"\n",
        "            if self.model is None:\n",
        "                raise ValueError(\"Model has not been trained yet.\")\n",
        "\n",
        "            return self.model.predict(x_test)\n",
        "    def predict_with_uncertainty(self, x, n_iter=100):\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model has not been trained. Call train() before making predictions.\")\n",
        "\n",
        "        predictions = []\n",
        "        for _ in range(n_iter):\n",
        "            preds = self.model.predict(x, batch_size=32, verbose=0)\n",
        "            predictions.append(preds)\n",
        "        predictions = np.array(predictions)\n",
        "\n",
        "        mean_prediction = predictions.mean(axis=0)\n",
        "        uncertainty = predictions.std(axis=0)\n",
        "\n",
        "        mean_prediction_reshaped = mean_prediction.reshape(-1, 1)\n",
        "        mean_prediction_scaled = self.scaler.inverse_transform(\n",
        "            np.hstack([mean_prediction_reshaped, np.zeros((mean_prediction_reshaped.shape[0], len(self.feature_names)-1))])\n",
        "        )[:, 0].reshape(mean_prediction.shape)\n",
        "\n",
        "        uncertainty_scaled = uncertainty * self.scaler.scale_[0]\n",
        "\n",
        "        return mean_prediction_scaled, uncertainty_scaled\n",
        "original_lstm_model = LSTMModel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4Baw9a9oG5o"
      },
      "source": [
        "## 4. Downloading Stock Data\n",
        "\n",
        "The first step in building our stock prediction model is to download historical stock data. In this section, we will:\n",
        "1. Define the parameters for the stock data we want to download (ticker symbol, start date, and end date).\n",
        "2. Create an instance of the `StockPredictor` class, which encapsulates the logic for downloading and processing stock data.\n",
        "3. Use the `download_stock_data()` method to fetch historical stock prices for the specified ticker and date range.\n",
        "4. Display the first few rows of the downloaded data to ensure everything was retrieved correctly.\n",
        "\n",
        "### Code Explanation:\n",
        "\n",
        "- **Parameters**: We define the `ticker`, `start_date`, and `end_date` to specify which stock and what date range to download.\n",
        "- **Instance Creation**: We create an instance of the `StockPredictor` class, which will handle downloading the stock data.\n",
        "- **Download Stock Data**: The `download_stock_data()` method fetches the data from Yahoo Finance for the given parameters.\n",
        "- **Display Data**: Finally, we print the first few rows of the data to inspect it and verify that it was downloaded correctly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lw4rDfMboG5o",
        "outputId": "59d3f903-d45e-43cd-da6c-82e968eb46cb"
      },
      "outputs": [],
      "source": [
        "# Define parameters\n",
        "ticker = \"aapl\"\n",
        "start_date = '2020-01-01'\n",
        "end_date = '2023-12-31'\n",
        "\n",
        "# Create an instance of StockPredictor\n",
        "predictor = StockPredictor()\n",
        "\n",
        "# Download stock data\n",
        "df = predictor.download_stock_data(ticker, start_date, end_date)\n",
        "print(\"Stock data downloaded\")\n",
        "\n",
        "# Display the first few rows of the downloaded data\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ETo4RU2oG5o"
      },
      "source": [
        "## 5. Data Preparation: Splitting, Scaling, and Creating Sequences\n",
        "\n",
        "In this section, we prepare the data for training our LSTM model by performing the following steps:\n",
        "\n",
        "1. **Add Technical Indicators**: We enhance the dataset by adding various technical indicators, such as moving averages, RSI, and MACD.\n",
        "2. **Perform Sentiment Analysis**: We integrate sentiment data into the dataset using the `SentimentAnalyzer`.\n",
        "3. **Train-Test Split**: We split the dataset into training and testing sets, with 80% of the data used for training and 20% for testing.\n",
        "4. **Data Scaling**: We scale the features using `MinMaxScaler` to ensure that the data is normalized, which helps the LSTM model converge during training.\n",
        "5. **Creating Sequences for LSTM**: We convert the data into sequences of time steps that will be used as input for the LSTM model.\n",
        "6. **Inspecting Data Shapes**: We output the shapes of the training and testing data to verify that the data preparation process is correct.\n",
        "\n",
        "### Code Explanation:\n",
        "\n",
        "- **Technical Indicators**: We use the `StockPredictor` class to add technical indicators to the DataFrame.\n",
        "- **Sentiment Analysis**: Sentiment data is added to the DataFrame on a weekly basis using the `SentimentAnalyzer`.\n",
        "- **Train-Test Split**: The dataset is split into training and testing sets using an 80/20 split.\n",
        "- **Data Scaling**: We scale the relevant features using `MinMaxScaler`.\n",
        "- **Creating Sequences**: The data is transformed into sequences suitable for training an LSTM model, which requires sequential data as input.\n",
        "- **Inspecting Data Shapes**: We print the shapes of the training and testing datasets to ensure that they are correctly prepared for model training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vslJWRysoG5p",
        "outputId": "4989c129-970b-4219-f659-f8035ad3ac24"
      },
      "outputs": [],
      "source": [
        "# Ensure predictor is initialized (if not done already)\n",
        "predictor = StockPredictor(look_back=60)  # Adjust look_back as needed\n",
        "\n",
        "# Add technical indicators\n",
        "print(\"Adding technical indicators...\")\n",
        "df = predictor.add_technical_indicators(df)\n",
        "print(\"Technical indicators added\")\n",
        "\n",
        "# Perform sentiment analysis\n",
        "sentiment_analyzer = SentimentAnalyzer(openai_client)\n",
        "print(\"Starting sentiment analysis...\")\n",
        "df = sentiment_analyzer.add_weekly_sentiment(df, ticker)\n",
        "print(\"Sentiment analysis completed\")\n",
        "\n",
        "# Scale the entire dataset\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_data = scaler.fit_transform(df[predictor.feature_names].values)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "train_size = int(len(scaled_data) * 0.8)\n",
        "\n",
        "# Prepare LSTM sequences\n",
        "print(\"Preparing data for LSTM...\")\n",
        "x_train, y_train = predictor.prepare_data(scaled_data[:train_size])\n",
        "x_test, y_test = predictor.prepare_data(scaled_data[train_size:])\n",
        "print(\"Data preparation completed\")\n",
        "\n",
        "# Print data shapes\n",
        "print(f\"x_train shape: {x_train.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"x_test shape: {x_test.shape}, y_test shape: {y_test.shape}\")\n",
        "\n",
        "# Verify that test set is not empty\n",
        "if x_test.size == 0 or y_test.size == 0:\n",
        "    raise ValueError(\"Test set is empty. Please check your data splitting and preparation steps.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pku1RT54oG5p"
      },
      "source": [
        "## 6. Model Initialization, Training, and Evaluation with Bayesian Optimization and Error Handling\n",
        "\n",
        "In this section, we focus on initializing, training, and evaluating the LSTM model, with added support for Bayesian optimization to find the optimal hyperparameters. We also include robust error handling to ensure the smooth execution of the training and evaluation processes.\n",
        "\n",
        "### Steps Involved:\n",
        "\n",
        "1. **Model Initialization**: We initialize the LSTM model by defining its architecture, which includes multiple LSTM layers and dropout for regularization.\n",
        "2. **Bayesian Optimization for Hyperparameter Tuning**: We perform Bayesian optimization to automatically find the best hyperparameters for the LSTM model, such as the number of LSTM units, dropout rate, and learning rate.\n",
        "3. **Model Training**: After finding the optimal hyperparameters, we retrain the LSTM model on the training data using these optimized settings.\n",
        "4. **Model Evaluation and Prediction**: Once training is complete, we make predictions on the test data and calculate performance metrics such as Mean Squared Error (MSE) and Mean Absolute Error (MAE). Additionally, we can estimate prediction uncertainty using Monte Carlo Dropout.\n",
        "5. **Error Handling**: A `try-except` block is used throughout the training and optimization process to catch and handle any errors that may arise, ensuring the script continues execution without crashing.\n",
        "\n",
        "### Code Explanation:\n",
        "\n",
        "- **Model Initialization**: The LSTM model is initialized using the `LSTMModel` class, which includes multiple LSTM layers with dropout for regularization. The model architecture can be customized through hyperparameters.\n",
        "  \n",
        "- **Bayesian Optimization**: The `train()` method leverages Bayesian optimization (`gp_minimize`) to search for the optimal hyperparameters. The optimization process minimizes the validation loss by testing different configurations of LSTM units, dropout rates, and learning rates.\n",
        "\n",
        "- **Model Training**: After finding the optimal hyperparameters, the model is retrained on the full training dataset using these settings for 200 epochs.\n",
        "\n",
        "- **Model Evaluation and Prediction**: Predictions are made on the test data, and we calculate metrics such as MSE and MAE to evaluate performance. Additionally, `predict_with_uncertainty()` can be used to provide uncertainty estimates for the predictions, which can be useful for understanding the model's confidence.\n",
        "\n",
        "- **Error Handling**: The `try-except` blocks catch any errors during the model training or evaluation process. If an error occurs during Bayesian optimization or training, it is handled gracefully, and a relevant error message is printed. This prevents the script from crashing and allows the optimization process to continue.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pxFEuHUt5_Vz",
        "outputId": "5b647d97-f6c8-4d95-ca27-214ed2b86a75"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    # Step 1: Initialize the LSTM model\n",
        "    original_lstm_model = LSTMModel()\n",
        "\n",
        "    # Step 2: Data Preparation - Scale the dataset using MinMaxScaler\n",
        "    scaler = MinMaxScaler()\n",
        "    scaled_data = scaler.fit_transform(df[original_lstm_model.feature_names].values)\n",
        "\n",
        "    # Split the data into training and testing sets (80% training, 20% testing)\n",
        "    train_size = int(len(scaled_data) * 0.8)\n",
        "    train_data = scaled_data[:train_size]\n",
        "    test_data = scaled_data[train_size:]\n",
        "\n",
        "    # Prepare the data for LSTM (create sequences for input)\n",
        "    x_train, y_train = original_lstm_model.prepare_data(train_data)\n",
        "    x_test, y_test = original_lstm_model.prepare_data(test_data)\n",
        "\n",
        "    # Reshape the target data (y_train, y_test) for the model's output format\n",
        "    y_train = y_train.reshape(-1, 1)  # Reshape to (batch_size, 1)\n",
        "    y_test = y_test.reshape(-1, 1)    # Reshape to (batch_size, 1)\n",
        "\n",
        "    # Step 3: Bayesian Optimization for Hyperparameter Tuning\n",
        "\n",
        "    original_lstm_model.train(x_train, y_train)\n",
        "\n",
        "except Exception as e:\n",
        "    # Step 5: Error Handling - Catch any errors in data preparation and print an error message\n",
        "    print(f\"An error occurred during data preparation: {str(e)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psH0fpSDBScL"
      },
      "source": [
        "## 7. Debugging and Validating Input Data for LSTM Model\n",
        "\n",
        "Before proceeding with model training, it's crucial to validate and debug the input data to ensure that it is properly formatted and free of common issues such as `NaN` values, incorrect shapes, or empty datasets. These issues can cause the model to fail during training or produce unreliable predictions.\n",
        "\n",
        "In this section, we will:\n",
        "\n",
        "1. **Define the `debug_input_data` Function**: This function inspects the input data for any potential issues. It checks for `NaN` values, incorrect data shapes, and empty datasets. Where possible, the function will fix minor issues (e.g., replacing `NaN` values) and raise errors for more significant problems.\n",
        "2. **Apply the Function**: We will apply the `debug_input_data` function to the training and testing datasets before feeding them into the LSTM model. This ensures that the data is in the correct format and meets the necessary requirements for training.\n",
        "3. **Handle Exceptions**: We use a `try-except` block to catch any exceptions that may arise during data debugging and preparation. If an issue is detected, an appropriate error message will be displayed, and the script will not proceed until the issue is resolved.\n",
        "\n",
        "### Code Explanation:\n",
        "\n",
        "- **Defining `debug_input_data`**: The function checks for `NaN` values and replaces them with zeroes, verifies that the dataset is not empty, and ensures that the input data for the LSTM model has the correct shape (3D: `[samples, time steps, features]`).\n",
        "- **Applying the Function**: We apply the `debug_input_data` function to `x_train`, `y_train`, `x_test`, and `y_test` to ensure that the data is suitable for model training.\n",
        "- **Handling Errors**: If any issues are detected during data validation, an error message is printed, and the process is halted. This helps prevent the model from training on bad data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UajEtg7T6B9o",
        "outputId": "901a8612-ce8e-414a-f626-402180483749"
      },
      "outputs": [],
      "source": [
        "# Define the utility function to debug and validate input data\n",
        "def debug_input_data(x_train, y_train, x_test, y_test):\n",
        "    \"\"\"\n",
        "    This function inspects and debugs the input data for common issues such as NaN values,\n",
        "    incorrect shapes, and empty datasets. It will try to fix minor issues and raise errors for major problems.\n",
        "\n",
        "    Args:\n",
        "        x_train (np.array): Training data features\n",
        "        y_train (np.array): Training data labels\n",
        "        x_test (np.array): Testing data features\n",
        "        y_test (np.array): Testing data labels\n",
        "\n",
        "    Returns:\n",
        "        tuple: Returns the (x_train, y_train, x_test, y_test) after debugging.\n",
        "    \"\"\"\n",
        "    # Check for NaN values in the data\n",
        "    if np.isnan(x_train).any() or np.isnan(y_train).any():\n",
        "        print(\"Warning: NaN values found in the training data. Replacing NaNs with 0.\")\n",
        "        x_train = np.nan_to_num(x_train)\n",
        "        y_train = np.nan_to_num(y_train)\n",
        "\n",
        "    if np.isnan(x_test).any() or np.isnan(y_test).any():\n",
        "        print(\"Warning: NaN values found in the test data. Replacing NaNs with 0.\")\n",
        "        x_test = np.nan_to_num(x_test)\n",
        "        y_test = np.nan_to_num(y_test)\n",
        "\n",
        "    # Check if the datasets are empty\n",
        "    if x_train.size == 0 or y_train.size == 0:\n",
        "        raise ValueError(\"Training set is empty. Please check your data preparation steps.\")\n",
        "\n",
        "    if x_test.size == 0 or y_test.size == 0:\n",
        "        raise ValueError(\"Test set is empty. Please check your data preparation steps.\")\n",
        "\n",
        "    # Ensure that the LSTM input data has the correct shape (3D array: [samples, time steps, features])\n",
        "    if len(x_train.shape) != 3:\n",
        "        raise ValueError(f\"Incorrect shape for x_train: {x_train.shape}. Expected a 3D array.\")\n",
        "\n",
        "    if len(x_test.shape) != 3:\n",
        "        raise ValueError(f\"Incorrect shape for x_test: {x_test.shape}. Expected a 3D array.\")\n",
        "\n",
        "    print(\"Input data successfully debugged and verified.\")\n",
        "\n",
        "    return x_train, y_train, x_test, y_test\n",
        "\n",
        "# Apply the debug_input_data function and handle any exceptions\n",
        "try:\n",
        "    # Debug input data to check for issues\n",
        "    x_train, y_train, x_test, y_test = debug_input_data(x_train, y_train, x_test, y_test)\n",
        "\n",
        "    # Ensure that the LSTM input has the correct shape\n",
        "    print(\"Input shape for LSTM:\")\n",
        "    print(f\"x_train shape: {x_train.shape}\")\n",
        "    print(f\"x_test shape: {x_test.shape}\")\n",
        "\n",
        "    # Check if the test set is empty\n",
        "    if x_test.size == 0:\n",
        "        print(\"Test set is still empty. Try adjusting the split ratio or inspecting the data preparation logic.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during data inspection: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JH4QbPiNDgRK"
      },
      "source": [
        "## 8. Making Predictions and Evaluating Model Performance\n",
        "\n",
        "After training the original LSTM model, the next step is to make predictions on the test data and evaluate the model's performance. In this section, we will:\n",
        "\n",
        "1. **Make Predictions**: Use the trained LSTM model to make predictions on the test dataset (`x_test`).\n",
        "2. **Inverse Transform Predictions and Actual Values**: Since the data was scaled during preprocessing, we need to inverse transform the predictions and actual values to return them to their original scale.\n",
        "3. **Calculate Performance Metrics**: Evaluate the model's performance using metrics such as Mean Squared Error (MSE) and Mean Absolute Error (MAE). These metrics provide insight into the accuracy of the model's predictions.\n",
        "\n",
        "### Code Explanation:\n",
        "\n",
        "- **Making Predictions**: The `predict` method of the trained LSTM model is called to generate predictions on the test data.\n",
        "- **Inverse Transforming Data**: Both the predictions and the actual values (`y_test`) are inverse transformed from the scaled values back to their original scale using the `scaler`. This step is crucial for accurately calculating performance metrics.\n",
        "- **Calculating Performance Metrics**: We compute the MSE and MAE to quantify the model's prediction accuracy. MSE gives us a sense of the average squared difference between predictions and actual values, while MAE measures the average absolute difference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUX4iii1r5cX",
        "outputId": "f20e1849-a400-4ad7-f186-f4026d4b8359"
      },
      "outputs": [],
      "source": [
        "# Make predictions on the test set using the trained LSTM model\n",
        "predictions = original_lstm_model.predict(x_test)\n",
        "\n",
        "# Inverse transform the predictions back to their original scale\n",
        "# We add zeros for the other features that were scaled but not predicted, to maintain the correct shape for inverse transformation\n",
        "predictions_scaled = np.hstack([predictions, np.zeros((len(predictions), len(original_lstm_model.feature_names)-1))])\n",
        "predictions_unscaled = scaler.inverse_transform(predictions_scaled)[:, 0]\n",
        "\n",
        "# Inverse transform the actual test values (y_test) back to their original scale\n",
        "# Similar to the predictions, we add zeros to maintain the correct shape for inverse transformation\n",
        "y_test_scaled = np.hstack([y_test, np.zeros((len(y_test), len(original_lstm_model.feature_names)-1))])\n",
        "y_test_unscaled = scaler.inverse_transform(y_test_scaled)[:, 0]\n",
        "\n",
        "# Calculate performance metrics to evaluate the model's accuracy\n",
        "mse = mean_squared_error(y_test_unscaled, predictions_unscaled)\n",
        "mae = mean_absolute_error(y_test_unscaled, predictions_unscaled)\n",
        "\n",
        "# Print the calculated performance metrics to assess the model's performance\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "print(f\"Mean Absolute Error: {mae}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6KvJEllfxaR"
      },
      "source": [
        "## 9. Inspecting Shapes and Model Features\n",
        "\n",
        "After making predictions and calculating performance metrics, it's important to inspect the shapes of the actual and predicted values, as well as the number of features used by the model. Ensuring that the shapes and feature counts are consistent is crucial for debugging and verifying that the model is processing data as expected.\n",
        "\n",
        "### Steps Involved:\n",
        "\n",
        "1. **Inspecting Shapes of `y_test` and Predictions**: We print the shapes of the actual test values (`y_test`) and the predicted values to verify that they match. Consistent shapes are necessary for calculating performance metrics and ensuring that the model is functioning correctly.\n",
        "2. **Inspecting Number of Features**: We print the number of features used by the model. This helps confirm that the model is processing the correct number of input features, which is essential for ensuring that all relevant data is being considered during training and prediction.\n",
        "\n",
        "### Code Explanation:\n",
        "\n",
        "- **Inspecting Shapes**: We print the shapes of `y_test` and `predictions` to check if they match. Any discrepancies in these shapes would indicate an issue in the data preparation or model prediction steps.\n",
        "- **Inspecting Features**: We print the number of features used by the model, which helps verify that the correct features were included during training and prediction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZ-DgV9wqoCS",
        "outputId": "a6dba4a5-7ee8-4e03-937f-25d8c906497c"
      },
      "outputs": [],
      "source": [
        "# Inspect and print the shape of the actual test values (y_test) to verify its dimensions\n",
        "print(\"Shape of y_test:\", y_test.shape)\n",
        "\n",
        "# Inspect and print the shape of the predicted values to ensure it matches the shape of y_test\n",
        "print(\"Shape of predictions:\", predictions.shape)\n",
        "\n",
        "# Print the number of features used by the original LSTM model during training and prediction\n",
        "print(\"Number of features:\", len(original_lstm_model.feature_names))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNhAKqLZD-PL"
      },
      "source": [
        "## 10. Rescaling Predictions and Plotting Stock Prices\n",
        "\n",
        "In this section, we rescale the predicted and actual stock prices back to their original range and visualize the results using a line plot. Rescaling the data is necessary because the data was previously scaled during preprocessing, and we need to convert it back to the original scale for meaningful interpretation and comparison. We also print relevant statistics, such as the date range and price range.\n",
        "\n",
        "### Steps Involved:\n",
        "\n",
        "1. **Get Original Price Range**: Extract the minimum and maximum values of the stock prices from the training data (`df['Close']`). These values are needed to rescale the data back to its original range.\n",
        "2. **Define Rescaling Function**: Create a function that converts the scaled data back to the original price range using the previously extracted minimum and maximum values.\n",
        "3. **Rescale Data**: Apply the rescaling function to both the actual test values (`y_test`) and the predicted values (`predictions`) to convert them back to the original scale.\n",
        "4. **Plot the Results**: Plot the rescaled actual and predicted prices against the test dates to visualize the model's performance. We customize the plot with labels, legends, and an improved x-axis format for better readability.\n",
        "5. **Print Statistics**: Display relevant statistics such as the date range, adjusted price range, and the original data's price range.\n",
        "\n",
        "### Code Explanation:\n",
        "\n",
        "- **Rescaling Function**: We define a function, `rescale_to_original`, that takes scaled data and rescales it to the original price range using the minimum and maximum prices.\n",
        "- **Plotting**: The actual and predicted prices are plotted on the same chart to visually compare the model's performance. The plot is customized with a title, labels, and a grid, and the x-axis date formatting is improved for better readability.\n",
        "- **Printing Statistics**: We print the date range, adjusted price range (after rescaling), and the original price range to provide additional context about the data being plotted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 758
        },
        "id": "P3HJTdoepyVD",
        "outputId": "31c3644a-7f82-4b34-e998-b3ed098a1c45"
      },
      "outputs": [],
      "source": [
        "# Get the original price range from your training data\n",
        "original_min_price = df['Close'].min()\n",
        "original_max_price = df['Close'].max()\n",
        "\n",
        "# Function to rescale the data back to original price range\n",
        "def rescale_to_original(scaled_data, original_min, original_max):\n",
        "    return scaled_data * (original_max - original_min) + original_min\n",
        "\n",
        "# Rescale y_test and predictions\n",
        "y_test_unscaled = rescale_to_original(y_test, original_min_price, original_max_price)\n",
        "predictions_unscaled = rescale_to_original(predictions, original_min_price, original_max_price)\n",
        "\n",
        "# Create test_dates (adjust the start date as needed)\n",
        "start_date = datetime(2023, 1, 1)  # Replace with your actual start date\n",
        "test_dates = [start_date + timedelta(days=i) for i in range(len(y_test))]\n",
        "\n",
        "# Now plot with the corrected data\n",
        "plt.figure(figsize=(15, 7))\n",
        "plt.plot(test_dates, y_test_unscaled, label='Actual Price', color='blue')\n",
        "plt.plot(test_dates, predictions_unscaled, label='Predicted Price', color='red')\n",
        "\n",
        "plt.title(f'{ticker} Stock Price Prediction')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Price ($)')\n",
        "plt.legend()\n",
        "\n",
        "# Improve x-axis date formatting\n",
        "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
        "plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=1))\n",
        "plt.gcf().autofmt_xdate()\n",
        "\n",
        "max_price = max(np.max(y_test_unscaled), np.max(predictions_unscaled))\n",
        "min_price = min(np.min(y_test_unscaled), np.min(predictions_unscaled))\n",
        "plt.ylim(min_price * 0.9, max_price * 1.1)  # Add 10% padding on both ends\n",
        "\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print statistics\n",
        "print(f\"Date range: {test_dates[0].strftime('%Y-%m-%d')} to {test_dates[-1].strftime('%Y-%m-%d')}\")\n",
        "print(f\"Adjusted price range: ${min_price:.2f} to ${max_price:.2f}\")\n",
        "print(f\"Original data adjusted price range: ${original_min_price:.2f} to ${original_max_price:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZQsshBTHxAE"
      },
      "source": [
        "## 11. Inspecting Data Structure and Samples\n",
        "\n",
        "In this final section, we inspect the structure and data types of the test set (`y_test`) and the model's predictions. Understanding the structure of your data is crucial for debugging and ensuring that the data is in the expected format. Additionally, printing a sample of the data allows for a quick verification that everything is working as expected.\n",
        "\n",
        "### Steps Involved:\n",
        "\n",
        "1. **Inspecting the Number of Features**: We print the number of features used by the model to ensure that the correct features were included during training and prediction.\n",
        "2. **Inspecting Shapes**: We check the shapes of `y_test` and `predictions` to verify that they match, which is necessary for accurate performance evaluation.\n",
        "3. **Inspecting Data Samples**: Printing the first few elements of both `y_test` and `predictions` allows for a quick visual inspection of the data and model output, helping to catch any obvious errors.\n",
        "4. **Inspecting Data Types**: We check the data types of `y_test` and `predictions` to confirm that they are the expected types (e.g., `numpy.ndarray`). Ensuring the correct data type is important for compatibility with various operations and functions.\n",
        "\n",
        "### Code Explanation:\n",
        "\n",
        "- **Number of Features**: The number of features used by the model is printed to verify that the model is working with the correct input data.\n",
        "- **Shapes**: We print the shapes of `y_test` and `predictions` to ensure that they are consistent and that there are no mismatches.\n",
        "- **Sample Data**: Printing the first five elements of `y_test` and `predictions` gives a quick overview of what the actual and predicted values look like, allowing for basic validation.\n",
        "- **Data Types**: The data types of `y_test` and `predictions` are printed to ensure they are compatible with further operations in the workflow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9qs69nsqvOI",
        "outputId": "8f5f19b7-ca91-417c-9e5c-4eb0d8ee7e4c"
      },
      "outputs": [],
      "source": [
        "print(\"Number of features:\", len(original_lstm_model.feature_names))\n",
        "print(\"Shape of y_test:\", y_test.shape)\n",
        "print(\"Shape of predictions:\", predictions.shape)\n",
        "print(\"Sample of y_test (first 5 elements):\", y_test[:5])\n",
        "print(\"Sample of predictions (first 5 elements):\", predictions[:5])\n",
        "print(\"Type of y_test:\", type(y_test))\n",
        "print(\"Type of predictions:\", type(predictions))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
